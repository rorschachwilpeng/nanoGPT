{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 语料库数据"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:11.472284Z",
     "start_time": "2024-08-21T14:26:11.464230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir='./data/shakespeare_input.txt'\n",
    "with open(dir,'r',encoding='utf-8') as f:\n",
    "    text=f.read()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:11.477151Z",
     "start_time": "2024-08-21T14:26:11.474321Z"
    }
   },
   "source": "print(len(text))#整个语料库大小",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:11.498437Z",
     "start_time": "2024-08-21T14:26:11.478688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#here are the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f'vocabulary size:{vocab_size}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocabulary size:65\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 分词，训练集/验证集划分"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 分词\n",
    "这里用的是最简单的分词-字符级标记器（character-level tokenizer），创建两个字典用于存储映射关系。第一个字典存放字符->索引下标映射；第二个字典存放索引下标->字符映射。\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 拓展知识点\n",
    "在造分词器的时候，可以权衡codebook size（词汇表）和序列长度。\n",
    "可以拥有词汇量非常小的非常长的整数序列，也可以拥有词汇量非常大的短整数序列。\n",
    "？？？\n",
    "1. 词汇量非常小的非常长的整数序列：\n",
    "\n",
    "这指的是使用一个较小的词汇表，但允许序列（即文本中的单词或标记序列）有较长的长度。在这种情况下，每个词或标记可能由一个较大的整数来表示，因为词汇表中的项较少，所以可以使用较大的数值范围。\n",
    "这种方法可能适用于某些特定的应用场景，比如当文本数据具有高度的重复性，或者当模型需要处理非常长的文本序列时。\n",
    "2. 词汇量非常大的短整数序列：\n",
    "\n",
    "这指的是使用一个较大的词汇表，但限制序列的长度。在这种情况下，每个词或标记由一个较小的整数来表示，因为词汇表中的项较多，所以每个项的表示范围较小。\n",
    "这种方法可能更适用于处理多样化的文本数据，因为它能够捕捉到更多的词汇细节，但同时也需要考虑到模型的内存和计算效率，因为较大的词汇表可能会增加模型的复杂度。\n",
    "--------------\n",
    "个人理解\n",
    "\n",
    "如果词汇表很大的话，那么（极端情况下）每个词/字符/字节都会有对应的索引下标。这样会造成一句话的序列可能会很长，所以一般当文本很短的时候，我们可以用大词汇表，小序列长度的方式，增加对词汇细微差别的理解。例如：社交媒体文本通常包含大量的俚语、表情符号和个性化词汇。在这种情况下，可能需要一个较大的词汇表来捕捉这些多样化的表达方式，但由于内存和计算资源的限制，序列长度可能需要被限制。\n",
    "\n",
    "如果词汇表很小的话，序列长度很长的话。往往是用于一些重复率比较高的长文本。例如：生物信息学，\n",
    "在处理基因序列数据时，序列长度可以非常长，但使用的“词汇”（即核苷酸）只有四种（A、T、C、G）。这里，序列长度是关键，而词汇量非常小。\n",
    "\n",
    "--------------\n",
    "Q:如果在特定场景应用了不恰当的分词策略，会造成什么影响？\n",
    "\n",
    "Q:对话大模型在实际应用中会根据用户输入的字符长度来动态选择分词策略吗？"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:11.504090Z",
     "start_time": "2024-08-21T14:26:11.500758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###Tokenizers\n",
    "# create a mapping from characters to integers\n",
    "str2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2str = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [str2idx[c] for c in s]\n",
    "decode = lambda l: ''.join(idx2str[c] for c in l)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:11.508786Z",
     "start_time": "2024-08-21T14:26:11.505596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在实践中，类似的分词策略例如：Google利用SentencePiece，即一种子词类型的标记器。该标记器没有对整个单词进行编码，也没有对单个字符进行编码。它是一个子字单元级别。\n",
    "\n",
    "OpenAI用了一个称为TickToken的库，它使用字节对编码标记器"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:13.340153Z",
     "start_time": "2024-08-21T14:26:11.510122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将整个数据集进行编码\n",
    "import torch\n",
    "data = torch.tensor(encode(text),dtype=torch.long)#这两个参数是什么意思？#这个函数在干嘛？\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/apple/Library/r-miniconda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/21/l9ygcw6d6w39m02vcn2t_j6r0000gn/T/ipykernel_56937/633919154.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/apple/Library/r-miniconda/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([ 0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
      "        44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
      "        63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,\n",
      "         1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39,\n",
      "        49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15,\n",
      "        47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50,\n",
      "        50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1,\n",
      "        58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51,\n",
      "        47, 57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43,\n",
      "        42,  8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1,\n",
      "        63, 53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56,\n",
      "        41, 47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51,\n",
      "        63,  1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0,\n",
      "        13, 50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,\n",
      "         1, 49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,\n",
      "         1, 46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39,\n",
      "        60, 43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,\n",
      "         1, 54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56,\n",
      "        42, 47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56,\n",
      "        43,  1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43,\n",
      "        58,  1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,\n",
      "         6,  1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45,\n",
      "        53, 53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56,\n",
      "        57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,\n",
      "         1, 39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47,\n",
      "        58, 47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41,\n",
      "        47, 39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59,\n",
      "        58, 46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53,\n",
      "        52,  1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57,\n",
      "        10,  1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47,\n",
      "        43, 50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54,\n",
      "        43, 56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,\n",
      "         1, 61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61,\n",
      "        43,  1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,\n",
      "         1, 56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52,\n",
      "        43, 50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52,\n",
      "        49,  1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,\n",
      "         1, 58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,\n",
      "         0, 39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1,\n",
      "        53, 40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43,\n",
      "        56, 63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52,\n",
      "        58, 53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56,\n",
      "        47, 57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41,\n",
      "        43, 11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1,\n",
      "        47, 57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1,\n",
      "        24, 43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47,\n",
      "        57,  1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1,\n",
      "        43, 56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43,\n",
      "        57, 10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52,\n",
      "        53, 61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,\n",
      "         1, 46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,\n",
      "         1, 52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,\n",
      "         1, 56, 43, 60, 43, 52, 45, 43,  8,  0])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 划分训练集/验证集"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:13.344728Z",
     "start_time": "2024-08-21T14:26:13.341498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]#训练集\n",
    "val_data = data[n:]#验证集"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:13.349245Z",
     "start_time": "2024-08-21T14:26:13.346292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n",
      "111540\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据加载器：批量数据块\n",
    "\n",
    "当我们训练Transformer时，我们不会将所有的语料一次性喂给模型。\n",
    "\n",
    "我们从训练集中随机抽取一个训练块进行训练，这些块的长度称为block_size"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:13.356691Z",
     "start_time": "2024-08-21T14:26:13.350314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#假设block_size为8，第一个block为\n",
    "block_size=8\n",
    "train_data[:block_size+1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上面的代码中，之所以block_size要+1是因为：当你从训练集中抽取一块数据时，比如上述的例子中有9个字符，其实包含了8个示例。每个例子如下： \n",
    "\n",
    "为什么这里Karpathy提到输入的是Transformer的张量的时间维度"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:13.364743Z",
     "start_time": "2024-08-21T14:26:13.359757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_data[:block_size]#前block_size的元素\n",
    "y = train_data[1:block_size+1]#这里为什么是从1开始，为什么要到\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([0]) the target: 18\n",
      "when input is tensor([ 0, 18]) the target: 47\n",
      "when input is tensor([ 0, 18, 47]) the target: 56\n",
      "when input is tensor([ 0, 18, 47, 56]) the target: 57\n",
      "when input is tensor([ 0, 18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([ 0, 18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([ 0, 18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([ 0, 18, 47, 56, 57, 58,  1, 15]) the target: 47\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Batch size\n",
    "每次我们要将它们输入Transformer时，我们都会有许多批次的多个文本块，它们都堆叠在一个张量(tensor)中，这样做的原因是为了提高效率，以便利用GPU的并行计算能力来并行处理数据。这些块是完全独立处理的，彼此之间不通信。\n",
    "\n",
    "Every time we're going to feed them into a transformer, we're going to have many batches of multiple chunks of text that are all stacked up in a single tensor.\n",
    "\n",
    "所以单个tensor包含了多个batch（批次），每个批次包含了多个chunks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:29:16.534653Z",
     "start_time": "2024-08-21T14:29:16.517959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split =='train' else val_data#输入训练数据/验证数据\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))#通过随机偏移量来选择不同的数据块\n",
    "    #生成偏移量的逻辑：在0-(len(data)-block_size)的范围内，随机生成batch_size个\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])#把这些随机抽取的序列进行拼接成batch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('input:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('target:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "for b in range(batch_size):#batch dimension\n",
    "    for t in range(block_size):#time dimension\n",
    "        context=xb[b,:t+1]\n",
    "        target=yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 0, 24, 43, 58,  5, 57,  1, 46],\n",
      "        [ 1, 44, 53, 56,  1, 58, 46, 39],\n",
      "        [43, 52, 58,  1, 58, 46, 39, 58],\n",
      "        [27, 25, 17, 27, 10,  0, 21,  1]])\n",
      "target:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "---------------\n",
      "when input is [0] the target: 24\n",
      "when input is [0, 24] the target: 43\n",
      "when input is [0, 24, 43] the target: 58\n",
      "when input is [0, 24, 43, 58] the target: 5\n",
      "when input is [0, 24, 43, 58, 5] the target: 57\n",
      "when input is [0, 24, 43, 58, 5, 57] the target: 1\n",
      "when input is [0, 24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [0, 24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [1] the target: 44\n",
      "when input is [1, 44] the target: 53\n",
      "when input is [1, 44, 53] the target: 56\n",
      "when input is [1, 44, 53, 56] the target: 1\n",
      "when input is [1, 44, 53, 56, 1] the target: 58\n",
      "when input is [1, 44, 53, 56, 1, 58] the target: 46\n",
      "when input is [1, 44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [1, 44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [43] the target: 52\n",
      "when input is [43, 52] the target: 58\n",
      "when input is [43, 52, 58] the target: 1\n",
      "when input is [43, 52, 58, 1] the target: 58\n",
      "when input is [43, 52, 58, 1, 58] the target: 46\n",
      "when input is [43, 52, 58, 1, 58, 46] the target: 39\n",
      "when input is [43, 52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [43, 52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [27] the target: 25\n",
      "when input is [27, 25] the target: 17\n",
      "when input is [27, 25, 17] the target: 27\n",
      "when input is [27, 25, 17, 27] the target: 10\n",
      "when input is [27, 25, 17, 27, 10] the target: 0\n",
      "when input is [27, 25, 17, 27, 10, 0] the target: 21\n",
      "when input is [27, 25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [27, 25, 17, 27, 10, 0, 21, 1] the target: 54\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 最简单的基线：二元语法模型，损失，生成\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:27:21.755616Z",
     "start_time": "2024-08-21T15:27:21.730679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        #token embedding table -> 标记嵌入表\n",
    "        #nn.Embedding -> 一个非常薄的包装器，基本上是一个形状为vocab_size*vocab_size的张量\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):#这是一个封装起来的强制函数\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #（B,T,C),在本例中,B(batch)批次为4，T(time)为8，C(chanel)为vocabSize即65\n",
    "        return logits\n",
    "        \n",
    "        #但是这里没有调用logits函数啊，怎么计算的呢\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "out\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
       "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
       "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
       "         ...,\n",
       "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
       "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
       "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761]],\n",
       "\n",
       "        [[ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
       "         [ 1.0541,  1.5018, -0.5266,  ...,  1.8574,  1.5249,  1.3035],\n",
       "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
       "         ...,\n",
       "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
       "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
       "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701]],\n",
       "\n",
       "        [[ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
       "         [-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
       "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
       "         ...,\n",
       "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
       "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
       "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305]],\n",
       "\n",
       "        [[-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
       "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
       "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
       "         ...,\n",
       "         [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
       "         [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
       "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T15:24:51.805502Z",
     "start_time": "2024-08-21T15:24:51.796883Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `forward` not found.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
